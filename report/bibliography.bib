%
% $Id$ 
%
% $LastChangedDate$ 
% 
% $LastChangedBy$
%

@article{RefWorks:68,
        author={Robert Cooley and Bamshad Mobasher and Jaideep Srivastava},
        year={1999},
        title={Data Preparation for Mining World Wide Web Browsing Patterns},
        journal={KNOWLEDGE AND INFORMATION SYSTEMS},
        volume={1},
        pages={5-32},
        abstract={The World Wide Web (WWW) continues to grow at an astounding rate in both the sheer volume of traffic and the size and complexity of Web sites. The complexity of tasks such as Web site design, Web server design, and of simply navigating through a Web site have increased along with this growth. An important input to these design tasks is the analysis of how a Web site is being used. Usage analysis includes straightforward statistics, such as page access frequency, as well as more sophisticated forms of analysis, such as finding the common traversal paths through a Web site. Web Usage Mining is the application of data mining techniques to usage logs of large Web data repositories in order to produce results that can be used in the design tasks mentioned above. However, there are several preprocessing tasks that must be performed prior to applying data mining algorithms to the data collected from server logs. This paper presents several data preparation techniques in order to identify unique users and user sessions. Also, a method to divide user sessions into semantically meaningful transactions is defined and successfully tested against two other methods. Transactions identified by the proposed methods are used to discover association rules from real world data using the WEBMINER system [15].},
        keywords={web; web\_mining},
}


@article{RefWorks:69,
        author={Anil K. Jain},
        year={2010},
        month={6/1},
        title={Data clustering: 50 years beyond K-means},
        journal={Pattern Recognition Letters},
        volume={31},
        number={8},
        pages={651-666},
        abstract={Organizing data into sensible groupings is one of the most fundamental modes of understanding and learning. As an example, a common scheme of scientific classification puts organisms into a system of ranked taxa: domain, kingdom, phylum, class, etc. Cluster analysis is the formal study of methods and algorithms for grouping, or clustering, objects according to measured or perceived intrinsic characteristics or similarity. Cluster analysis does not use category labels that tag objects with prior identifiers, i.e., class labels. The absence of category information distinguishes data clustering (unsupervised learning) from classification or discriminant analysis (supervised learning). The aim of clustering is to find structure in data and is therefore exploratory in nature. Clustering has a long and rich history in a variety of scientific fields. One of the most popular and simple clustering algorithms, K-means, was first published in 1955. In spite of the fact that K-means was proposed over 50 years ago and thousands of clustering algorithms have been published since then, K-means is still widely used. This speaks to the difficulty in designing a general purpose clustering algorithm and the ill-posed problem of clustering. We provide a brief overview of clustering, summarize well known clustering methods, discuss the major challenges and key issues in designing clustering algorithms, and point out some of the emerging and useful research directions, including semi-supervised clustering, ensemble clustering, simultaneous feature selection during data clustering, and large scale data clustering.},
        keywords={Data clustering; User’s dilemma; Historical developments; Perspectives on clustering; King-Sun Fu prize},
        isbn={0167-8655},
        note={{Although \textit{K-Means} was devised in 1955, it is still
                  widely used because of its simplicity, efficiency,
                  and empirical success. The paper looks at the
                  difficulties of developing better
                  algorithms. Clustering algorithms can be broadly
                  divided into \textit{hierarchical} and
                  \textit{partitional}. Hierarchical algorithms
                  recursively find nested clusters in either
                  \textit{agglomerative} (bottom up) or
                  \textit{divisive} (top down) mode, taking an ${n *
                  n}$ similarity matrix as input. Partitional
                  algorithms take as input an ${n * d}$ pattern matrix
                  or a similarity matrix.\\\\

                  The \textit{K-Means} algorithm finds a partition
                  such that the squared error between the empirical
                  mean of a cluster and the points in the cluster is
                  minimized. The goal is to minimize the squared error
                  over all clusters however this problem is NP-hard
                  so, out of necessity, \textit{K-Means} is a greedy
                  algorithm which converges to a local
                  minima. Research does show however that if the
                  clusters are well separated, the algorithm will
                  converge with high probability to the global
                  optimum. The main steps of \textit{K-Means} are\\
\begin{enumerate}
\item Select an initial partition and repeat steps 2 and 3 until cluster membership stabilizes.
\item Generate a new partition by assigning each pattern to its nearest cluster centre.
\item Compute new cluster centres.\\
\end{enumerate}

                  \textit{K-Means} requires three user-specified
                  parameters: number of clusters, cluster
                  initialization, and distance metric. Selection of
                  number of clusters is difficult and usually based on
                  heuristics and/or repeated execution with different
                  number of clusters, adjudicated by a domain
                  expert. \textit{K-Means} typically uses the
                  Euclidean distance metric and as a result, finds
                  hyperspherical shaped clusters.\\\\

                  Clustering algorithms have been developed that model
                  pattern density by a probabilistic mixture model
                  viz. EM algorithm and several Bayesian
                  approaches. These methods are attractive because of
                  their ability to deal with arbitrary shaped clusters
                  but have difficulty dealing with high dimensional
                  data the feature space is characteristically
                  sparse, making it difficult to distinguish high
                  density regions from low. Graph theoretic clustering
                  is another class of clustering algorithms. These
                  algorithms represents data points as nodes in a
                  graph with connecting edges weighted by their
                  pair-wise similarity. The central idea is to
                  partition the nodes into two groups such that the
                  weights of the edges between the two groups is
                  minimized.\\\\

                   All the variables involved in a clustering project
                  make it inherently difficult. One of the most
                  important decisions is that of data
                  representation. A good data representation will
                  result in compact, well separated clusters however
                  there is no universally good representation and the
                  process must be guided by domain knowledge. Another
                  variable is the number of clusters. Automatic
                  determination of this variable has been one of the
                  most difficult problems in
                  clustering. Alternatively, the optimal number of
                  clusters must be determined through trial and error.\\\\

                  Since clustering algorithms tend to find clusters
                  irrespective of whether they exist, it is important
                  to objectively evaluate whether the data has a
                  natural tendency to cluster. \textit{Cluster
                  validation} is the formal evaluation of clustering
                  results in a quantitative and objective
                  manner. Cluster validity measures can be
                  \textit{internal}, \textit{external}, or
                  \textit{relative}. Internal measures asses the fit
                  between the structure imposed by the algorithm and
                  the data itself. Relative measures compare the
                  structure imposed by different algorithms on the
                  same data. External measures compare cluster
                  structure to some a priori information, namely
                  "true" class labels.\\\\

                  Stability of a clustering solution is a measure of
                  how much variation occurs in the structure imposed
                  over different sub-samples drawn from the input
                  data. Different measures of variation can be used to
                  obtain different stability measures. Since many
                  algorithms are asymptotically stable, it may be
                  important to consider the rate at which stability is
                  reached.\\\\

                  Some recent clustering trends include:\\
\begin{itemize}
\item \textbf{Clustering Ensembles}: combine the resulting partitions resulting from application of differing clustering methods on the same data.
\item \textbf{Semi-Supervised Clustering}:  a subset of the data is labelled and these are used to impose pairwise constraints (\textit{must-link} and \textit{cannot-link}) on the cluster algorithm.
\item \textbf{Large-Scale Clustering}: algorithms developed to handle large data sets can be classified as: efficient nearest neighbour (NN), data summarization, distributed computing, incremental clustering, or sampling-based methods.
\end{itemize}
                  }}
}


@article{RefWorks:70,
        author={A. K. Jain and M. N. Murty and P. J. Flynn},
        year={1999},
        month={September},
        title={Data clustering: a review},
        journal={ACM Comput.Surv.},
        volume={31},
        number={3},
        pages={264-323},
        keywords={cluster analysis; clustering applications; exploratory data analysis; incremental clustering; similarity indices; unsupervised learning},
        isbn={0360-0300},
        note={Paper is an overview of data clustering concepts and
                  techniques. Clustering is an exploratory undertaking
                  (unsupervised), as opposed to classification
                  (supervised). During clustering, a collection of
                  patterns are organized based on a notion of their
                  similarity to one another. Patterns are typically
                  represented as feature vectors and their
                  organization occurs within this feature
                  space. Pattern selection involves feature selection
                  as well as feature extraction, transforming input
                  features to produce new salient features. All
                  clustering algorithms will produce clusters
                  regardless of the underlying data so how do we
                  evaluate a cluster algorithm ? Cluster validation
                  studies can be \textit{external}, comparing the
                  recovered structure to an \textit{a priori}
                  structure; \textit{internal}, which examines whether
                  the structure is intrinsically appropriate for the
                  data; or \textit{relative}, which compares two
                  structures and measures their relative merit.\\\\

                  A measure of the similarity between two patterns is
                  essential to most clustering procedures. The most
                  common measure is the Euclidean distance. It works
                  well when a data set has compact, isolated clusters
                  but large scale features tend to dominate unless
                  weighted or normalized. Salient cluster algorithm
                  properties include: agglomerative vs divisive;
                  monothetic vs polythetic; hard vs fuzzy;
                  deterministic vs stochastic; incremental vs
                  non-incremental; and hierarchical vs
                  partitioning.\\\\

                  Hierarchical algorithms produce a
                  \textit{dendrogram} representing nested groupings of
                  patterns and the similarity thresholds at which they
                  change. Most hierarchical cluster algorithms are
                  variants of the single-link, \textit{single-link}
                  (distance between clusters is the minimum between
                  any two patterns drawn from different clusters);
                  \textit{complete-link} (distance between clusters is
                  the maximum between any two patterns from different
                  clusters); and \textit{minimum-variance} algorithms.\\\\

                  Partitioning algorithms are less demanding
                  computationally compared to hierarchical
                  algorithms. A problem of these algorithms is the
                  choice of the number of desired output clusters. The
                  most common and intuitive criterion function used in
                  partitional clustering is the \textit{squared-error}
                  criterion of which \textit{K-Means} is the simplest
                  and most commonly used. \textit{K-Means} is one of
                  the most efficient in terms of execution time and
                  one of the few methods appropriate for use on large
                  data sets. \textit{K-Means} requires one to specify
                  the number of clusters to create which is difficult
                  to do optimally. Variants of \textit{K-Means} have
                  been proposed which dynamically merge and/or spilt
                  clusters based on a variance threshold.\\\\

                  Clusters are typically represented by their
                  centroid, a simple scheme if clusters are compact
                  and iso-tropic. If clusters are elongated or
                  non-isotropic, then this representation weak, better
                  replaced by a collection of points.\\\\

                  Search based cluster techniques can be either
                  deterministic or stochastic. Deterministic
                  techniques guarantee an optimal partition by
                  performing exhaustive enumeration. Stochastic search
                  techniques generate near optimal partitions
                  reasonably quickly and guarantee asymptotic
                  convergence to optimal partition.\\\\

                  Clustering is subjective by nature. Subjectivity is
                  usually incorporated into some phase of clustering,
                  whether it be in selection of a pattern
                  representation, choosing a similarity measure, or
                  cluster representation. The incorporation of domain
                  knowledge consists of ad-hoc approaches with little
                  in common.\\\\

                  Clustering of large data sets is computationally
                  demanding and many clustering algorithms do not
                  scale adequately. The emerging discipline of data
                  mining has spurred developments and optimizations in
                  this area. Clustering is used in the data mining
                  process for segmentation of databases into
                  homogeneous groups, predictive modelling, and
                  visualization. If the data set is too large to fit
                  in main memory, techniques like
                  \textit{divide-and-conquer}, incremental clustering,
                  and parallel algorithm implementations have been
                  used.\\\\

                  The paper review several application domains in
                  which clustering has been successfully employed:
                  image segmentation, object and character
                  recognition, information retrieval, and data
                  mining.\\\\ 
        }
}


@article{RefWorks:71,
        author={Bing Liu},
        year={2010},
        title={{Sentiment Analysis: A Multifaceted Approach}},
        journal={IEEE Intell.Syst.IEEE Intelligent Systems},
        volume={25},
        number={3},
        pages={74-76},
        isbn={1541-1672},
        language={English},
        note={{The World Wide Web has made large numbers of
                  opinionated texts available, driving the
                  study of \textit{sentiment analysis}, a field which combines
                  problems from many different sub-fields and in which
                  significant progress has been made over the last few
                  years. \\\\

                  Originally, research treated the problem of
                  sentiment analysis as one of text classification:
                  \textit{Sentiment Classification} classifies a
                  document as expressing a positive or negative
                  opinion; \textit{Subjectivity Classification}
                  aims to determines whether a sentence is subjective
                  or objective. This treatment has since been expanded
                  to encompass more detailed analysis required for
                  many real-world applications. In particular, users
                  are interested in determining the subject of an
                  opinion.\\\\

                  In defining the sentiment analysis or opinion mining
                  problem, we make the following definitions:\\

\begin{itemize}
\item \textbf{opinion target}: the target entity or object about which
                  an opinion is being expressed. An opinion target can
                  have a set of components, and/or attributes about
                  which an opinion can be expressed, in addition to
                  the object itself.
\item \textbf{opinion holder}: the entity expressing the opinion.
\item \textbf{opinion}: a positive or negative appraisal of an opinion
                  target. Positives and negatives are called the
                  orientation of the opinion. Opinions may be direct
                  or comparative.
\end{itemize}

                  \bigskip
                  These aspects combine to form the \textit{feature
                  based sentiment analysis model}. In this model,
                  opinionated documents are analyzed to extract the
                  following information.\\

\begin{itemize}
\item \textbf{opinion quintuples}: capture orientation of an opinion
                  expressed regarding a particular object feature by a
                  particular opinion holder at a specific time. 
\item \textbf{synonyms} of each object feature.
\end{itemize}

                  \bigskip
                  For opinion extraction, existing approaches are
                  based on different supervised and unsupervised
                  methods using opinion words and phases and grammar
                  information. This task is difficult because of the
                  scope of how opinions may be expressed across
                  different domains and between different opinion
                  holders.\\\\

                  Correlating the attributes of the opinion quintuples
                  requires a high level of
                  integration. \textit{Natural language processing}
                  (NLP) techniques have been applied to this task but
                  even within this well defined field there are many
                  aspects to which accurate solutions have not been
                  discovered viz. co-reference resolution and word-sense
                  disambiguation.\\\\
                  
                  In evaluating semantic analysis systems,
                  \textit{precision} and \textit{recall} are common
                  measures. In most applications high precision is
                  critical but high recall may not be necessary as
                  long as the system can extract enough opinions to
                  ensure a statistical balance of errors and not
                  destroy the natural distribution of sentiment.\\\\

                  In practise, completely automated solutions are not
                  imminent however it is possible to devise effective
                  semi-automated systems. \\\\
                  }}
}


@misc{RefWorks:72,
        author =         {Magnus Rosell},
        year =   {2006},
        title =          {{Introduction to Information Retrieval and Text Clustering}},
        abstract =       {Information Retrieval (IR) is a large and growing field within Natural Language Processing (NLP). The search engine is the most well-known (and perhaps still the only really useful) application. Search engines like Google and AltaVista are used by many people on a daily basis. There are several other applications within IR. Among them this text considers text clustering in particular. A text clustering algorithm partitions a set of texts so that texts within the same group are as similar in content as possible. It is done without using any predefined categories. Text clustering can for instance be applied to the documents retrieved by a search engine, so that they can be presented in groups according to content.},
        note = {{This is a collection of chapters adopted from the
                  authors licentiate theses \textit{Clustering in
                  Swedish}. The first chapter introduces the field of
                  Information Retrieval (IR), as a large and growing
                  field within Natural Language Processing (NLP). IR
                  is the theoretical foundation of text search
                  engines. Texts are represented as vectors, each
                  dimension corresponding to a distinct word in the
                  set of words appearing in all texts. The vector
                  fields are weights which model how important the
                  corresponding word is deemed to be in the context of
                  the text. There are many weighting schemes but in
                  the most common the weights are the product the
                  \textit{term-frequency} (tf) and \textit{inverse
                  document frequency} (idf). The term frequency is a
                  function of the number of occurrences of a particular
                  word in a document divided by the number of words in
                  the entire document. The inverse document frequency
                  models the distinguishing power of the word in the
                  text set; the fewer documents that contain the word,
                  the more information about the text int he text set
                  it gives.\\\\

                  In a text query, a search is conducted for texts
                  similar to the search vector which is represented in
                  the same way as the texts. The most common measure
                  of similarity is the \textit{cosine measure}, the
                  cosine of the angle between the query and texts. The
                  texts are returned are ranked by similarity. It is
                  difficult to evaluate search results. In a
                  controlled text set, query results can be compared
                  against results of human opinion. By comparing these
                  perspectives, we may define performance measures for
                  the search engine. \textit{Precision} and
                  \textit{recall} are common measures. To further
                  characterize search engine performance over a range
                  of operating conditions, the precision at different
                  levels of recall can be plotted in a graph.\\\\

                  Modifications can be made to the vector space model
                  described to improve search performance:\\
\begin{itemize}
\item \textbf{Stoplist and Word Classes}: stoplist words are excluded from the model, usually very common words whose occurrence do not separate one text significantly from another.
\item \textbf{Phases}: treat phrases as separate dimensions for phrase based searches.
\item \textbf{Lemmatizing and Stemming}: extracting word fragments that appear frequently in documents.
\item \textbf{Related Words}: the vector model does not account for the fact that words may be related (synonyms, homonyms etc). Many attempts have been made to attempt to address this phenomenon viz. word sense disambiguation, query expansion.
\item \textbf{Statistically Related Words}: statistical examination of the word-by-document matrix gives information regarding words that appear together often. This information can be used by search engines to improve performance. \textit{Latent Semantic Analysis} (LSA) is such a technique but is computationally heavy. \textit{Random Indexing} (RI) is a much faster, less memory intensive alternative but does not use the entire word-by-document matrix.
\item \textbf{Meta-data}: meta-data found in web pages provides additional information that can be used when indexing.\\
\end{itemize}
                  \bigskip
                  Text clustering can be used to discover structures
                  within a text set that were not previously
                  known. This is as opposed to text categorization
                  where texts are assigned to predefined
                  categories. IR and text clustering are related in
                  that they both employ the same pattern
                  representation and search function. Researchers
                  believe that credible text clustering could make
                  search times shorter by retrieving clusters of texts
                  instead of individual documents. Similar (clustered)
                  documents are probably relevant to the same queries
                  but that does not mean that pre-clustering of the
                  entire text set can take all future queries into
                  account ( I think this means that clustering is
                  coarse grained relative to search queries). The
                  authors argue for text clustering after ordinary
                  search engine retrieval and have shown through
                  experimentation that this can improve search result
                  quality.\\\\

                  It is hard to objectively evaluate clustering
                  results since the value thereof is subjective. It is
                  common to distinguish between intrinsic and external
                  measures. Intrinsic measure use no external
                  knowledge other than what was available to the
                  cluster algorithm. External measures use external
                  knowledge.\\\\
             }}
}


@article{RefWorks:73,
        author={G. P. Zhang},
        year={2000},
        title={Neural networks for classification: a survey},
        journal={Systems, Man, and Cybernetics, Part C: Applications and Reviews, IEEE Transactions on},
        volume={30},
        number={4},
        pages={451-462},
        note={{This paper is a review of the use of Artificial Neural
                  Networks (ANN) for classification tasks. It compares
                  ANNs to statistically based classification
                  procedures and explains advantages and disadvantages
                  of ANN relative to these more traditional
                  approaches. The paper shows how ANNs are able to
                  estimate posterior classification probabilities by
                  virtue of the fact that ANNs are typically trained
                  by attempting to minimize mean squared errors. This
                  provides a direct link between ANN classifications
                  and statistical methods, particularly Bayesian.\\\\
                  Direct comparison of ANN and statistical classifiers
                  may not be possible because ANNs are non-linear and
                  model-free, while statistical methods are linear and
                  model-based. However, by appropriately encoding ANN
                  outputs, we can use ANNs to directly model some high
                  order discriminant functions. Analysis along these
                  lines has shown that the hidden layers of an MLP
                  project the data onto different clusters in a way
                  that these clusters can be further aggregated into
                  different classes. However, the added flexibility of
                  ANNs due to hidden layers does not automatically
                  guarantee their superiority over logistical
                  regression due to possible overfitting and other
                  inherent problems.\\\\

                  Due to the variables associated with constructing
                  ANN classifiers and the local minima problem
                  associated with training ANNs, there is an inherent
                  error between true posterior probabilities and the
                  least square estimates provided by ANN. This prediction error
                  is composed of two components, the
                  \textit{approximation error} and the
                  \textit{estimation error}. The \textit{approximation
                  error} reflects a inherent irreducible consequence
                  of the randomness of the training data. The
                  \textit{estimation error} is a reflection of the
                  effectiveness of the ANN to approximate the target
                  function.\\\\

                  The paper describes how a bias-plus-variance
                  decomposition of the ANN prediction error provides
                  useful information on how the estimate differs from
                  the target function. The model bias quantifies how
                  the average estimates over all possible data sets of
                  the same size differ from the target function. Bias
                  is an indication of the limitations of the model
                  itself. Model variance is an indication of the
                  sensitivity of the estimation function to the
                  training data set. Bias and variance are generally
                  conflicting goals. ANNs are flexible and tend to
                  have low bias but high variance.\\\\
                  Ensemble methods are described where classifiers are
                  combined by averaging or voting prediction results
                  from multiple ANNs. Improvements in prediction
                  results are attributed to reduction of variance. The
                  technique seems to work best when the voting models
                  disagree with one another strongly i.e. are
                  biased. Averaging seems to offset this bias and
                  reduce sensitivity to the data. Methods of
                  constructing biased models include statistical
                  resampling techniques and using different feature
                  variables.\\\\

                  Feature selection methods for ANNs are mostly
                  heuristic in nature and and lack statistical
                  justification.\\\\

                  Taking misclassification costs into account seems to
                  improve the performance of ANNs in terms of
                  classification and feature selection. Various
                  techniques are described for incorporating
                  misclassification cost information and prior
                  knowledge of relative class importance, however,
                  little research has been done in the this area.\\\\
                  }},
        abstract={Classification is one of the most active research and application areas of neural networks. The literature is vast and growing. This paper summarizes some of the most important developments in neural network classification research. Specifically, the issues of posterior probability estimation, the link between neural and conventional classifiers, learning and generalization trade off in classification, the feature variable selection, as well as the effect of misclassification costs are examined. Our purpose is to provide a synthesis of the published research in this area and stimulate further research interests and efforts in the identified topics},
        keywords={generalization (artificial intelligence); learning (artificial intelligence); neural nets; pattern classification; classification; conventional classifiers; feature variable selection; generalization; learning; misclassification costs; neural classifiers; neural networks; posterior probability estimation},
        isbn={1094-6977}
}


@inbook{RefWorks:74,
        author={Ali Ghorbani and I. Onut},
        editor={Graña Romay,Manuel and Corchado,Emilio and Garcia Sebastian,M.},
        year={2010},
        title={Y-Means: An Autonomous Clustering Algorithm},
        series={Hybrid Artificial Intelligence Systems},
        publisher={Springer Berlin / Heidelberg},
        volume={6076},
        pages={1-13},
        abstract={This paper proposes an unsupervised clustering technique for data classification based on the K-means algorithm. The K-means algorithm is well known for its simplicity and low time complexity. However, the algorithm has three main drawbacks: dependency on the initial centroids, dependency on the number of clusters, and degeneracy. Our solution accommodates these three issues, by proposing an approach to automatically detect a semi-optimal number of clusters according to the statistical nature of the data. As a side effect, the method also makes choices of the initial centroid-seeds not critical to the clustering results. The experimental results show the robustness of the Y-means algorithm as well as its good performance against a set of other well known unsupervised clustering techniques. Furthermore, we study the performance of our proposed solution against different distance and outlier-detection functions and recommend the best combinations.},
        note={{The paper describes a new clustering
                  technique, \textit{Y-Means}, based on the
                  seminal \textit{K-Means} algorithm. \textit{Y-Means}
                  addresses three main limitations of
                  the \textit{K-Means} method:\\

\begin{itemize}
\item \textbf{Dependence on choice of initial centroids}: \textit{K-Means} is based on the mean squared error and converges to a local minima. In \textit{Y-Means} the final result is independent of the choice of initial centroids. \textbf{WHY ?}

\item \textbf{Dependence number of centroids}: finding the optimal number of centroid is NP-hard. \textit{Y-Means} aims to find a semi-optimal approximation by exploiting statistical properties of the data. \textit{Y-Means} starts with an arbitrary number of clusters and iteratively splits clusters based on an outlier detection function. Once cluster structure has stabilized, clusters may be merged based on a merging threshold.

\item \textbf{Degeneracy}: there is no mechanism in \textbf{K-Means} to eliminate empty clusters at the end of the clustering process. \textit{H-Means+} and \textit{X-Means} are \textit{K-Means} variants which attempt to deal with the degeneracy issue.\\\\
\end{itemize}

                  \bigskip
                  \textit{Y-Means} begins by normalizing the data set
                  to remove the dominating effect of large-scale
                  features. Then, an arbitrary number of cluster
                  centroids are randomly chosen and the algorithm
                  enters an iterative loop until the cluster
                  stabilizes. At the start of each iteration
                  \textit{K-Means} is executed and empty clusters are
                  eliminated from the resulting structure. Then an
                  outlier detection function is applied to each
                  cluster and outliers are removed to form new cluster
                  centres. This process repeats until the cluster
                  structure stabilizes. Finally, similar clusters are
                  merged based on a merging threshold and the
                  resulting clusters are labelled. Labelling is domain
                  dependent and only used when it applies to the data
                  set. During experimentation, the authors used
                  \textit{size-based} and \textit{distance-based}
                  labelling which are specific to the intrusion
                  detection domain.\\\\

                  Various outlier identification functions were used,
                  based on Mahalanobis, Tukey, and Radusu Based
                  metrics. The authors experimented with two popular
                  statistical rules for outlier threshold definition:
                  the \textit{Empirical Rule} (assumes a normal
                  distribution) and the \textit{Chebyshev's
                  Inequality} (applies to any kind of
                  distribution). Six different point-to-point distance
                  metrics were used in \textit{Y-Means} experiments:
                  Euclidean, Manhattan, Minkowski of order 3,
                  Chebyshev, Canberra, and Pearson's Coefficient of
                  Correlation.\\\\

                  Merging of two clusters occurs if the distance
                  between their centroids is not greater than a
                  threshold. As with cluster splitting, this threshold
                  is based on the statistical distribution of each
                  cluster. The threshold is calculated as a weighted
                  sum of the ${\sigma}$ of two clusters being
                  considered for merging. \textit{Y-Means} uses the
                  \textit{linking} technique to merge clusters,
                  creating multi-centroid clusters which better model
                  the data as opposed to \textit{fusing} which
                  combines centroids to form a new one.\\\\

                  In experiments using the KDD Cup 1999 data set,
                  \textit{Y-Means} exhibited good performance compared
                  with four well known unsupervised algorithms: EM,
                  K-Means, SOM, and ICLN.\\\\
    }}                  
}


@book{RefWorks:75,
        author={Tom M. Mitchell},
        year={1997},
        title={Machine Learning},
        publisher={McGraw-Hill},
        address={New York},
        isbn={0070428077 9780070428072 0071154671 9780071154673},
        language={English}
}


@inproceedings{RefWorks:76,
        author={Lijuan Zhou and Linshuang Wang and Xuebin Ge and Qian Shi},
        year={2010},
        title={A clustering-Based KNN improved algorithm CLKNN for text classification},
        booktitle={Informatics in Control, Automation and Robotics (CAR), 2010 2nd International Asia Conference on},
        volume={3},
        pages={212-215},
        abstract={As a simple, effective and nonparametric classification method, k Nearest Neighbour (KNN) is widely used in document classification for dealing with the much more difficult problem such as large-scale or many of categories. But KNN classifier may have a problem when training samples are uneven. The problem is that KNN classifier may decrease the precision of classification because of the uneven density of training data. To solve the problem, a new clustering-based KNN method is presented in this paper. It preprocesses training data by using clustering, then classify with a new KNN algorithm, which adopts a dynamic adjustment in each iteration for the neighbourhood number K. This method would avoid the uneven classification phenomenon and reduce the misjudgement of the boundary testing samples. We have an experiment in text classification and the result shows that it has good performance.},
        isbn={1948-3414},
        note={{The KNN classification algorithm is widely used in
                  large scale text classification
                  applications. Although many other classification
                  algorithms have been devised over the years, KNN
                  scales better than most. KNN does not, however,
                  perform well where training samples are unevenly
                  distributed within the feature space. This paper
                  proposes a new algorithm, \textit{CLKNN}, that
                  performs pre-processing of training data sets using
                  unsupervised clustering to even out training data
                  distribution before applying KNN classification.\\\\

                  Previously, in dealing with the problem of unevenly distributed
                  training samples, a density reduction technique has
                  been shown to improve the speed and accuracy of
                  KNN. However, this technique does not address the
                  issue of low density regions. In \textit{CLKNN}, a
                  clustering algorithm is applied (which clustering
                  algorithm exactly is not mentioned in the paper) to
                  the training data set to partition it into a number
                  of small mutually exclusive neighbourhoods. If the
                  number of samples in a cluster exceeds a certain
                  threshold and they all belong to the same class, the
                  cluster centroid is substituted in the training data
                  to represent all samples in the cluster. This
                  process evens out the training data which is then
                  processed by modified KNN algorithm.\\\\

                  Experimental results show that \textit{CLKNN}
                  exhibits an improvement over regular KNN
                  classification accuracy and execution
                  time. Experiments however only used two types of
                  data sets and more work needs to be done to validate
                  the effect of this technique.\\\\

                  The language in this paper is poor and the logic
                  difficult to follow.\\\\
        }}
}


@inproceedings{RefWorks:77,
        author={Sheng Sun and YuanZhen Wang},
        year={2010},
        title={K-Nearest Neighbour Clustering Algorithm Based on Kernel Methods},
        booktitle={Intelligent Systems (GCIS), 2010 Second WRI Global Congress on},
        volume={3},
        pages={335-338},
        abstract={KNN algorithm is the most usable classification algorithm, it is simple, straight and effective. But KNN can not identify the effect of attributes in dataset. For non-Gaussian distribution or non-Elliptic distribution, KNN can not solve these two kinds of problem effectively. A major approach to tackle this problem is to give each of the rest of attributes a weight value according to the relationship between these attributes. The bigger the attribute weight is, it has more importance extent in figuring out the distance of samples in kernel space. In this paper, we proposed a kernel-based KNN clustering algorithm which improved accuracy of KNN clustering algorithm. We tested the accuracy rate of the suggested algorithm KKNNC using the six UCI data sets, and compared it with KNNC algorithm in the experiments. The experimental results show that KKNNC algorithm outperform KNNC algorithm in accuracy significantly.}
}


@article{RefWorks:78,
        author={Isabelle Guyon and Andr\'e Elisseeff},
        year={2003},
        month={March},
        title={{An Introduction to Variable and Feature Selection}},
        journal={J.Mach.Learn.Res.},
        volume={3},
        pages={1157-1182},
        isbn={1532-4435},
        note={{Variable and feature selection are meant to improve
                  the accuracy and speed of predictors and to provide
                  a better understanding of the underlying process
                  that produced the data. The paper describes various
                  techniques and methods used in the process of
                  variable and feature selection:\\\\

\textbf{Variable Ranking}\\

                  Variable ranking is a \textit{filter} method applied
                  during preprocessing and independent of the choice
                  of predictor. It is simple, scalable, and exhibits
                  good empirical success. Statistically it is robust
                  against overfitting because it introduces bias.\\\\

                  A scoring or correlation function is computed based
                  on the features and used to sort variables. To use
                  variable ranking to build predictors, nested subsets
                  incorporating progressively more variables of
                  decreasing relevance are defined.\\\\

                  Using a correlation criteria like \textit{Pearson's
                  Coefficient}, one can only detect linear
                  dependencies between variable and target
                  (supervised). This restriction may be lifted by
                  making a non-linear fit of the target with single
                  variables and rank according to the goodness of
                  fit. Overfitting can be avoided by using non-linear
                  preprocessing and then using a simple correlation
                  coefficient. \\\\

                  As opposed to using a correlation function for
                  variable ranking, variables can be selected
                  according to their individual predictive power,
                  using as criterion the performance of a classifier
                  built with that variable alone.\\\\

                  Several approaches to variable selection using
                  information theoretic criteria have been
                  proposed. Many rely on mutual information between
                  each variable the the target (supervised).\\\\
               
                  The paper presents some informative examples that
                  highlight the shortcoming of variable ranking
                  techniques which evaluate variables predictive power
                  individually:\\
\begin{itemize}
\item \textbf{Can presumably redundant variables help each other
                  out?}: noise reduction and consequently better class
                  separation may be obtained by adding variables that
                  are presumably redundant.
\item \textbf{How does variable correlation impact variable
                  redundancy?}: perfectly correlated variables are
                  truly redundant in the sense that no additional
                  information is gained by adding them. Very high
                  variable correlation (or anti-correlation) does not
                  mean that said variables cannot complement one
                  another. 
\item \textbf{Can a variable that is useless by itself be useful with
                  others?}: yes it can; also, two variables that are
                  useless by themselves can be useful together.
\end{itemize}

\bigskip
\textbf{Variable Subset Selection}\\

                  Variable subset selection considers the predictive
                  power of groups of variables as opposed to
                  individually. Such techniques are divided into
                  \textit{wrappers}, \textit{filters}, and
                  \textit{embedded methods}.\\

\begin{itemize}
\item \textbf{Filters}: filters select subsets of variables as a
                  pre-processor step, independently of the chosen
                  predictor. Filters are faster than wrappers but not
                  tuned to a specific learning machine. Filters can be
                  used to reduce space dimensionality and overcome
                  overfitting. 

\item \textbf{Wrappers}: use the learning machine of interest to
                  score variable subsets according to their predictive
                  power. An exhaustive search through the variable
                  subset space is conceivable but NP-hard. A wide
                  range of search strategies can be used to prevent the
                  search becoming computationally intractable. These
                  methods are universal and simple.

\item \textbf{Embedded Methods}: perform variable selection in the
                  process of training and usually specific to the
                  learning machine. They make better use of available
                  data and reach a solution faster by avoiding
                  retraining the predictor from scratch for every
                  variable subset investigated.
\end{itemize}

\bigskip
\textbf{Feature Construction and Space Dimensionality Reduction}\\

                  Feature construction is concerned with improving
                  predictor performance and building more compact
                  feature subsets. Two distinct goals may be pursued
                  for feature construction: achieving best
                  reconstruction of the data (unsupervised) or being
                  most efficient for making predictions
                  (supervised). Feature construction is an opportunity
                  to incorporate domain knowledge into the model and
                  and can be very application specific, however there
                  are a number of generic techniques, some of which
                  are described:\\

\begin{itemize}
\item \textbf{Clustering}: a group of similar variables by a cluster
                  centroid, which becomes a feature. Some supervised
                  may be introduced to obtain more discriminant
                  features. Clustering is commonly used for feature
                  selection in text processing. Here the supervision
                  comes from a priori document categories. 

\item \textbf{Matrix Factorization}: \textit{singular value
                  decomposition} (SVD) forms sets of features that are
                  linear combinations of the original variables and
                  which provide the best possible reconstruction
                  thereof in the least square sense. The method is
                  unsupervised. 

\item \textbf{Supervised Feature Selection}: the paper reviews three
                  approaches for selecting features in cases where
                  features should be distinguished from variables
                  because both appear simultaneously in the system
                  (\textbf{WHAT DOES THIS MEAN ?})
\end{itemize}

\bigskip
\textbf{Validation Methods}
                  It is important to distinguish between the problem
                  of model selection and final evaluation of the
                  predictor. For predictor evaluation an independent
                  test should be kept aside. For model selection, the
                  remaining data should be further split between fixed
                  training and validation , or cross validation can be
                  used. Statistical tests can be used to estimate the
                  significance of differences in validation errors.\\\\

\textbf{Unsupervised Variable Selection}
                  In unsupervised variable selection, there are a
                  number of variable ranking criterion, a number of
                  which are useful across applications, including:
                  \textit{saliency}, \textit{entropy},
                  \textit{smoothness}, \textit{density}, and
                  \textit{reliability}.\\\\

\textbf{Forward vs Backward Selection}
                  Forward selection (add variables) is computationally
                  more efficient than backward selection (eliminate
                  variables). However, it is argued that forward finds
                  weaker subsets because the importance of variables
                  is not assessed within the context of other
                  variables not yet included.\\\\

        }}
}


@inproceedings{RefWorks:79,
        author={Yiming Yang and Jan O. Pedersen},
        year={1997},
        title={A Comparative Study on Feature Selection in Text Categorization},
        booktitle={Proceedings of the Fourteenth International Conference on Machine Learning},
        series={ICML '97},
        publisher={Morgan Kaufmann Publishers Inc},
        address={San Francisco, CA, USA},
        pages={412-420},
        isbn={1-55860-486-3},
        note={{The paper is a comparative study of five different
                  feature selection techniques as applied to the
                  problem of text categorization: \textit{document
                  frequency} (DF), \textit{information gain} (IG),
                  \textit{mutual information} (MI), \(\chi^2\)-test
                  (CHI), and \textit{term strength} (TS). All of these
                  techniques use a term-goodness criterion threshold
                  to achieve the desired degree of term elimination
                  from the full vocabulary of a document corpus. These
                  methods all fall into the wrapper class of feature
                  selection techniques, as opposed to filters or
                  embedded methods. To evaluate the effectiveness of
                  the feature selection in each case, two well known,
                  highly scalable, classification algorithms are used:
                  \textit{k-nearest neighbour} (KNN) and a regression method
                  named \textit{Linear Least Squares Fit} (LLSF). The choice of
                  classifiers is based on the fact that they differ
                  statistically and therefore should reduce classifier
                  bias in the results. Feature sets were evaluated by
                  measuring precision and recall over the Reuter-22173
                  collection and OHSUMED.\\\\

\begin{itemize}
\item \textbf{Document Frequency Thresholding:} document frequency of
                  a term is the number of documents in which a term
                  appears. The DF for each unique term in the training
                  corpus was calculated and those whose DF was below a
                  certain threshold were eliminated from the feature
                  space. The assumption here is that rare terms are
                  either non-informative for classification, or not
                  influential in global performance. Improvement in
                  accuracy is also possible if rare terms happen to be
                  noise terms.  DF is considered an ad-hoc approach
                  but scales well by virtue of its simplicity and
                  performs relatively well in this case. I think that
                  simple techniques, like this one especially, that
                  are purely lexical do not have much scope for
                  improvement. Although semantic analysis is
                  complicated and not well developed, it offers much
                  more scope for improving text processing in
                  general.

\item \textbf{Information Gain:} measures the amount of information
                  gained for classification with knowledge of the
                  presence or absence of a term in a document. Feature
                  terms whose information gain falls below a certain
                  threshold, are removed from the feature space.

\item \textbf{Mutual Information:} criterion commonly used in
                  statistical language modelling of word
                  associations. It is based on joint and marginal
                  probabilities of terms and categories in the training
                  corpus. It is strongly influenced by marginal term
                  probabilities, favouring rare terms.

\item \textbf{\(\chi^2\) Statistic:} measures the lack of independence
                  between term and document co-occurence. For each
                  category, CHI is calculated between each unique term
                  in the training corpus and that category, and then
                  the category specific scores are combined into two
                  scores for each term. Like MI, CHI has a strong
                  statistical basis however CHI is a normalized value
                  and can be compared across terms for the same
                  category. CHI is known not to be reliable for
                  low-frequency terms.

\item \textbf{Term Strength:} estimates term relevance based on how
                  likely term is to appear in closely related (by
                  \textit{cosine rule} thresholding) documents. This method is
                  radically different from the others. It is based on
                  document clustering, assuming that documents with
                  many shared words are similar.
\end{itemize}

\bigskip 
\textbf{Results}\\ 
                  DF, IG and CHI perform well and are strongly
                  correlated. DF's basis on common terms would
                  indicate that, contrary to popular belief, common
                  terms are often informative (perhaps this is
                  particular to text classification). Given that
                  removing stop words seems generally to improve
                  classification performance, I think there must be
                  some kind of occurence frequency threshold above
                  which terms no longer give useful information for
                  classification purposes. MI exhibited inferior
                  performance compared to other methods due to its
                  bias for rare terms and/or a strong sensitivity to
                  probability estimation errors.\\

                  Interestingly, MI is \textit{task- sensitive} (uses
                  category information) but under performs TS and DF
                  which are \textit{task-free}. It would seem that
                  using category information for feature selection is
                  not crucial for good performance.\\\\
        }}
}


@inproceedings{RefWorks:80,
        author={Charu C. Aggarwal and Jiawei Han and Jianyong Wang and Philip S. Yu},
        year={2003},
        title={A framework for clustering evolving data streams},
        booktitle={Proceedings of the 29th international conference on Very large data bases - Volume 29},
        series={VLDB '2003},
        publisher={VLDB Endowment},
        location={Berlin, Germany},
        pages={81-92},
        isbn={0-12-722442-4},
}


@article{RefWorks:81,
        author={Aoying Zhou and Feng Cao and Weining Qian and Cheqing Jin},
        year={2008},
        title={Tracking clusters in evolving data streams over sliding windows},
        journal={Knowledge and Information Systems},
        volume={15},
        number={2},
        pages={181-214},
        abstract={Mining data streams poses great challenges due to the limited memory availability and real-time query response requirement. Clustering an evolving data stream is especially interesting because it captures not only the changing distribution of clusters but also the evolving behaviours of individual clusters. In this paper, we present a novel method for tracking the evolution of clusters over sliding windows. In our SWClustering algorithm, we combine the exponential histogram with the temporal cluster features, propose a novel data structure, the Exponential Histogram of Cluster Features (EHCF). The exponential histogram is used to handle the in-cluster evolution, and the temporal cluster features represent the change of the cluster distribution. Our approach has several advantages over existing methods: (1) the quality of the clusters is improved because the EHCF captures the distribution of recent records precisely; (2) compared with previous methods, the mechanism employed to adaptively maintain the in-cluster synopsis can track the cluster evolution better, while consuming much less memory; (3) the EHCF provides a flexible framework for analyzing the cluster evolution and tracking a specific cluster efficiently without interfering with other clusters, thus reducing the consumption of computing resources for data stream clustering. Both the theoretical analysis and extensive experiments show the effectiveness and efficiency of the proposed method.},
        keywords={Computer Science},
        isbn={0219-1377}
}


@article{RefWorks:82,
        author={Mohamed Medhat Gaber and Arkady Zaslavsky and Shonali Krishnaswamy},
        year={2005},
        month={June},
        title={{Mining Data Streams: A Review}},
        journal={SIGMOD Rec.},
        volume={34},
        number={2},
        pages={18-26},
        isbn={0163-5808},
        note={{This paper reviews the theoretical foundations of data
                  stream analysis. Mining of data streams uses
                  techniques based on well established statistical and
                  computational approaches. These techniques can be
                  categorized into \textit{data-based} and
                  \textit{task-based}.\\\\

                  \textbf{Data-based} techniques
                  involve summarizing the whole data set or choosing a
                  subset thereof to analyze:\\

\begin{itemize}
\item \textbf{Sampling}: an old statistical technique involving the
                  probabilistic choice of whether to process a data
                  item or not. Boundaries of the error rate of the
                  computation are given as a function of time. In
                  stream analysis the unknown data set size requires
                  special analysis to derive this error bound
                  function. Sampling does not address the problem of
                  fluctuating data rates.
\item \textbf{Load Shedding}: involves dropping a sequence of data
                  streams. This makes the technique difficult to use
                  with mining techniques as the dropped portions may
                  contain patterns of interest in time series
                  analysis. Load shedding has been used successfully
                  in querying data streams but suffers from the same
                  problems as sampling.
\item \textbf{Sketching}: random projection of a subset of features
                  through vertical sampling of the input
                  stream. Sketching has been applied in comparing
                  different data streams and in aggregate queries but
                  it is hard to use in the context of data stream
                  mining; the major drawback is that of poor accuracy.
\item \textbf{Synopsis Data Structures}: applying summarization
                  techniques to create a synopsis of incoming data
                  suitable for further analysis. Wavelet analysis,
                  histograms, quantiles, and frequency moments have
                  been proposed as synopsis data structures. Since not
                  all aspects of the data set are retained during
                  summarization, approximate answers are produced.
\item \textbf{Aggregation}: computing statistical measures that
                  characterize the data stream and using these
                  measures with mining algorithms. Aggregation does
                  not perform well with highly fluctuating data
                  distributions.
\end{itemize}

                  \bigskip
                  \textbf{Task-based} techniques are methods which
                  modify existing or invent new techniques to
                  specifically address the computational challenges of
                  data stream processing:\\

\begin{itemize}
\item \textbf{Approximation Algorithms}: designing algorithms for
                  computationally hard problems which create
                  approximate solutions with error
                  bounds. Approximation algorithms have attracted
                  researchers as a direct solution to data stream
                  mining problems however, the technique does not
                  solve the problem of data rates with regard to
                  resource availability.
\item \textbf{Sliding Window}: detailed analysis is done over the most
                  recent data window and summarized versions of old
                  data. 
\item \textbf{Algorithm Output Granularity (AOG)}: the first
                  resource-aware data analysis approach that can cope
                  with fluctuating very high data rates. Data mining
                  followed by adaptation to resources and data stream
                  rates represents the first two stages of
                  AOG. Lastly, generated knowledge structures are
                  merged when running out of memory. (This looks like
                  a promising technique) 
\end{itemize}

                  \bigskip
                  A number of algorithms have been proposed for mining
                  of streaming data:\\\\ 

\textbf{Clustering}\\

                  Guha et al \cite{RefWorks:82.27,RefWorks:82.28} have
                  applied K-median in a divide and conquer
                  fashion. Fixed-size data samples are individually
                  clustered and the resulting cluster centres are
                  clustered. This process is repeated to a fixed
                  number of levels.\\\\

                  Babcock et al \cite{RefWorks:82.7} have used
                  exponential histogram (EH) data structures to
                  improve \cite{RefWorks:82.27} by addressing the
                  problem of merging clusters when the two sets of
                  cluster centres are far apart. Other k-median based
                  techniques have been proposed which overcome the
                  problem of increasing approximation factors with the
                  increase in the number of cluster aggregation
                  levels.\\\\

                  Domingos et al
                  \cite{RefWorks:82.15,RefWorks:82.16,RefWorks:82.35}
                  proposed Very Fast Machine Learning (VFML), a
                  general method for scaling up machine learning
                  algorithms. The method depends on determining an
                  upper bound on the the learning loss as a function
                  of the number of data items to be examined in each
                  step of the algorithm. VFML has been applied to
                  k-means clustering (VFKM) and decision tree
                  classification (VFDT).\\\\
                  
                  Ordonez \cite{RefWorks:82.46} proposed improvements
                  to k-means to cluster binary data streams. Also, an
                  incremental one-pass k-means variant has been
                  developed and demonstrated to outperform scalable
                  k-means in the majority of cases.\\\\
                  
                  O'Callaghan et al \cite{RefWorks:82.45} proposed
                  STREAM and LOCALSEARCH algorithms for high quality
                  data stream clustering.\\\\
                  
                  Aggarwal et al \cite{RefWorks:82.1} have proposed a
                  data stream clustering framework called CluStream
                  which divides the clustering process into an online
                  component, which summarizes data stream statistics,
                  and an offline component which clusters the
                  summarized data.\\\\
                  
                  Keogh et al \cite{RefWorks:82.39} have proved
                  empirically that most time series data clustering
                  algorithms proposed so far produce meaningless
                  results in subsequence clustering (what is this
                  ?). They propose a solution using k-motif to choose
                  subsequences that would provide meaningful
                  results.\\\\
                  
                  Gaber et al \cite{RefWorks:82.21} have developed
                  Lightweight Clustering (LWC), an AOG based algorithm
                  which is sensitive to resource availability.\\\\


\textbf{Classification}\\

                  Wang et al \cite{RefWorks:82.53} proposed an algorithm
                  to account for mining concept drifting data streams
                  (what is this ?) using weighted classifier
                  ensembles.\\\\
                  
                  Ganti et al \cite{RefWorks:82.18} developed
                  analytically an algorithm for model maintenance
                  under insertion and deletion of data records. The
                  algorithm can be applied to any incremental data
                  mining model (promising). Also, they have described
                  a general framework for change detection between two
                  data sets in terms of the data mining results they
                  induce (does it apply to data streams ?). These two
                  techniques are called GEM and FOCUS.\\\\
                  
                  Papdimitiou et al \cite{RefWorks:82.48} have
                  proposed a single-pass incremental algorithm for
                  pattern discovery from sensor data. The algorithm
                  uses wavelet coefficients as compact information
                  representation and correlation data structure
                  detection, and then apply a linear regression model
                  in the wavelet domain. (Ghorbani used wavelet
                  analysis for anomaly detection in IDS. Related
                  ?)\\\\
                  
                  Aggarwal et al \cite{RefWorks:82.3} have adapted the
                  idea of micro-clusters introduced in CluStream to
                  use clustering results to classify data using the
                  statistical class distribution in each cluster.\\\\
                  
                  Gaber et al \cite{RefWorks:82.21} have developed
                  Lightweight Classification (LWClass), a variation of
                  LWC.\\\\
                  
                  
\textbf{Frequency Counting}\\
                  
                  Gianella et al \cite{RefWorks:82.20} have developed
                  a frequent item sets mining algorithm over data
                  streams. They proposed the use of a tilted window to
                  calculate the frequent patterns for the most recent
                  transactions.
                  
                  Manku and Motwani \cite{RefWorks:82.43} have proposed
                  and implemented a frequency counting algorithm which
                  uses group testing to find the most frequent
                  items. The algorithm is used with the turnstile data
                  stream model which allows additions and deletion of
                  data items.
                  
                  Gaber et al \cite{RefWorks:82.21} have developed
                  Light Weight Frequency Counting (LWF), an AOG based
                  algorithm.\\\\


\textbf{Time Series Analysis}\\
                  
                  Indyk et al \cite{RefWorks:82.36} have proposed
                  approximate solutions with probabilistic error
                  bounding to the problems of relaxed periods and
                  queueing trends. The algorithms use dimensionality
                  reduction sketching techniques and have been shown
                  experimentally to be efficient in running time and
                  accuracy.\\\\
                  
                  Perlman and Java \cite{RefWorks:82.49} have proposed
                  an approach to mine astronomical time series
                  streams. Sliding window patterns are clustered and
                  an association rule technique used to create
                  affinity analysis results among the created
                  clusters.\\\\
                  
                  Zhu and Sasha \cite{RefWorks:82.54} have proposed
                  techniques to compute statistical measures using
                  discrete Fourier Transforms.\\\\
                  
                  Lin et al \cite{RefWorks:82.42} proposed using
                  symbolic representation to reduce dimensionality and
                  numerosity.\\\\
                  
                  Chen et al \cite{RefWorks:82.12} proposed the
                  application of multidimensional regression analysis
                  to create compact cubes that can be used for
                  answering aggregate queries over the incoming
                  streams.\\\\
                  
                  
\textbf{Research Issues}\\
                  
                  Handling the continuous flow of data is not a
                  capability native to most database management
                  systems. Novel indexing, storage and querying
                  techniques are required to handle this non-stop
                  fluctuated flow of data. Processing of data streams
                  has an unbounded memory requirement. This places
                  limits on the machine learning techniques that can
                  be applied since most methods require data to be
                  resident in memory while the algorithm runs. It is
                  important to design space efficient techniques that
                  can have only one look or less over an incoming
                  stream.\\\\
                  
                  How do we model the change of mining results over
                  time ? Dynamics of data structures using changes in
                  the knowledge structures generated would benefit
                  many temporal-based analysis applications. Also,
                  traditional mining algorithms do not produce results
                  that show the change of the results over time. We
                  need to develop algorithms for mining such
                  changes.\\\\
                  
                  Data stream preprocessing is a way of reducing the
                  amount of memory required and the amount of effort
                  to process a data stream. Light-weight preprocessing
                  algorithms that can guarantee the quality of the
                  mining results would be of great benefit.\\\\}}
}


@inproceedings{RefWorks:82.1,
 author = {Aggarwal, Charu C. and Han, Jiawei and Wang, Jianyong and Yu, Philip S.},
 title = {A framework for clustering evolving data streams},
 booktitle = {Proceedings of the 29th international conference on Very large data bases - Volume 29},
 series = {VLDB '2003},
 year = {2003},
 isbn = {0-12-722442-4},
 location = {Berlin, Germany},
 pages = {81--92},
 numpages = {12},
 acmid = {1315460},
 publisher = {VLDB Endowment},
} 

@inproceedings{RefWorks:82.3,
 author = {Aggarwal, Charu C. and Han, Jiawei and Wang, Jianyong and Yu, Philip S.},
 title = {On demand classification of data streams},
 booktitle = {Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining},
 series = {KDD '04},
 year = {2004},
 isbn = {1-58113-888-1},
 location = {Seattle, WA, USA},
 pages = {503--508},
 numpages = {6},
 acmid = {1014110},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {classification, data streams},
} 


@inproceedings{RefWorks:82.7,
 author = {Babcock, Brain and Datar, Mayur and Motwani, Rajeev and O'Callaghan, Liadan},
 title = {Maintaining variance and k-medians over data stream windows},
 booktitle = {Proceedings of the twenty-second ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems},
 series = {PODS '03},
 year = {2003},
 isbn = {1-58113-670-6},
 location = {San Diego, California},
 pages = {234--243},
 numpages = {10},
 acmid = {773176},
 publisher = {ACM},
 address = {New York, NY, USA},
} 


@inproceedings{RefWorks:82.12,
 author = {Chen, Yixin and Dong, Guozhu and Han, Jiawei and Wah, Benjamin W. and Wang, Jianyong},
 title = {Multi-dimensional regression analysis of time-series data streams},
 booktitle = {Proceedings of the 28th international conference on Very Large Data Bases},
 series = {VLDB '02},
 year = {2002},
 location = {Hong Kong, China},
 pages = {323--334},
 numpages = {12},
 acmid = {1287398},
 publisher = {VLDB Endowment},
} 


@inproceedings{RefWorks:82.15,
 author = {Domingos, Pedro and Hulten, Geoff},
 title = {Mining high-speed data streams},
 booktitle = {Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining},
 series = {KDD '00},
 year = {2000},
 isbn = {1-58113-233-6},
 location = {Boston, Massachusetts, United States},
 pages = {71--80},
 numpages = {10},
 acmid = {347107},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Hoeffding bounds, decision trees, disk-based algorithms, incremental learning, subsampling},
} 


@inproceedings{RefWorks:82.16,
 author = {Domingos, Pedro and Hulten, Geoff},
 title = {A General Method for Scaling Up Machine Learning Algorithms and its Application to Clustering},
 booktitle = {Proceedings of the Eighteenth International Conference on Machine Learning},
 series = {ICML '01},
 year = {2001},
 isbn = {1-55860-778-1},
 pages = {106--113},
 numpages = {8},
 acmid = {658293},
 publisher = {Morgan Kaufmann Publishers Inc.},
 address = {San Francisco, CA, USA},
} 


@article{RefWorks:82.18,
 author = {Ganti, Venkatesh and Gehrke, Johannes and Ramakrishnan, Raghu},
 title = {Mining data streams under block evolution},
 journal = {SIGKDD Explor. Newsl.},
 volume = {3},
 issue = {2},
 month = {January},
 year = {2002},
 issn = {1931-0145},
 pages = {1--10},
 numpages = {10},
 acmid = {507517},
 publisher = {ACM},
 address = {New York, NY, USA},
} 


@article{RefWorks:82.20,
	title = {Chapter 3 Mining Frequent Patterns in Data Streams at Multiple Time},
	author = {Chris Giannella and Jiawei Han and Jian Pei and Xifeng Yan and Philip S. Yu},
	year = {2007},
	keywords = {frequent pattern, data stream, stream data mining},
	institution = {CiteSeerX - Scientific Literature Digital Library and Search Engine [http://citeseerx.ist.psu.edu/oai2] (United States)},
}


@inproceedings{RefWorks:82.21,
 author = {Gaber, Mohamed Medhat and Shiddiqi, Ary Mazharuddin},
 title = {Distributed data stream classification for wireless sensor networks},
 booktitle = {Proceedings of the 2010 ACM Symposium on Applied Computing},
 series = {SAC '10},
 year = {2010},
 isbn = {978-1-60558-639-7},
 location = {Sierre, Switzerland},
 pages = {1629--1630},
 numpages = {2},
 acmid = {1774439},
 publisher = {ACM},
 address = {New York, NY, USA},
} 


@inproceedings{RefWorks:82.27,
 author = {Guha, S. and Mishra, N. and Motwani, R. and O'Callaghan, L.},
 title = {Clustering data streams},
 booktitle = {Proceedings of the 41st Annual Symposium on Foundations of Computer Science},
 year = {2000},
 isbn = {0-7695-0850-2},
 pages = {359--},
 acmid = {796588},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 keywords = {Web click stream analysis, computational complexity, constant-factor approximation algorithms, data analysis, data stream clustering, data stream model, deterministic algorithms, k-median problem, massive data sets, multimedia data analysis, pattern clustering, point sequence, very large databases},
} 


@article{RefWorks:82.28,
 author = {Guha, Sudipto and Meyerson, Adam and Mishra, Nina and Motwani, Rajeev and O'Callaghan, Liadan},
 title = {Clustering Data Streams: Theory and Practice},
 journal = {IEEE Trans. on Knowl. and Data Eng.},
 volume = {15},
 issue = {3},
 month = {March},
 year = {2003},
 issn = {1041-4347},
 pages = {515--528},
 numpages = {14},
 acmid = {776777},
 publisher = {IEEE Educational Activities Department},
 address = {Piscataway, NJ, USA},
 keywords = {Clustering, data streams, approximation algorithms.},
} 


@inproceedings{RefWorks:82.35,
 author = {Hulten, Geoff and Spencer, Laurie and Domingos, Pedro},
 title = {Mining time-changing data streams},
 booktitle = {Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining},
 series = {KDD '01},
 year = {2001},
 isbn = {1-58113-391-X},
 location = {San Francisco, California},
 pages = {97--106},
 numpages = {10},
 acmid = {502529},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Decision trees, Hoeffding bounds, concept drift, data streams, incremental learning, subsampling},
} 


@inproceedings{RefWorks:82.36,
 author = {Indyk, Piotr and Koudas, Nick and Muthukrishnan, S.},
 title = {Identifying Representative Trends in Massive Time Series Data Sets Using Sketches},
 booktitle = {Proceedings of the 26th International Conference on Very Large Data Bases},
 series = {VLDB '00},
 year = {2000},
 isbn = {1-55860-715-3},
 pages = {363--372},
 numpages = {10},
 acmid = {671699},
 publisher = {Morgan Kaufmann Publishers Inc.},
 address = {San Francisco, CA, USA},
} 


@inproceedings{RefWorks:82.39,
 author = {Keogh, Eamonn and Lin, Jessica and Truppel, Wagner},
 title = {Clustering of Time Series Subsequences is Meaningless: Implications for Previous and Future Research},
 booktitle = {Proceedings of the Third IEEE International Conference on Data Mining},
 series = {ICDM '03},
 year = {2003},
 isbn = {0-7695-1978-4},
 pages = {115--},
 acmid = {952156},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 keywords = {Time Series, Data Mining, Clustering, Rule Discovery},
} 


@inproceedings{RefWorks:82.42,
 author = {Lin, Jessica and Keogh, Eamonn and Lonardi, Stefano and Chiu, Bill},
 title = {A symbolic representation of time series, with implications for streaming algorithms},
 booktitle = {Proceedings of the 8th ACM SIGMOD workshop on Research issues in data mining and knowledge discovery},
 series = {DMKD '03},
 year = {2003},
 location = {San Diego, California},
 pages = {2--11},
 numpages = {10},
 acmid = {882086},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data mining, data streams, discretize, symbolic, time series},
} 


@inproceedings{RefWorks:82.43,
 author = {Manku, Gurmeet Singh and Motwani, Rajeev},
 title = {Approximate frequency counts over data streams},
 booktitle = {Proceedings of the 28th international conference on Very Large Data Bases},
 series = {VLDB '02},
 year = {2002},
 location = {Hong Kong, China},
 pages = {346--357},
 numpages = {12},
 acmid = {1287400},
 publisher = {VLDB Endowment},
} 


@inproceedings{RefWorks:82.45,
 author = {Ordonez, Carlos},
 title = {Clustering binary data streams with K-means},
 booktitle = {Proceedings of the 8th ACM SIGMOD workshop on Research issues in data mining and knowledge discovery},
 series = {DMKD '03},
 year = {2003},
 location = {San Diego, California},
 pages = {12--19},
 numpages = {8},
 acmid = {882087},
 publisher = {ACM},
 address = {New York, NY, USA},
} 


@inproceedings{RefWorks:82.46,
 author = {Ordonez, Carlos},
 title = {Clustering binary data streams with K-means},
 booktitle = {Proceedings of the 8th ACM SIGMOD workshop on Research issues in data mining and knowledge discovery},
 series = {DMKD '03},
 year = {2003},
 location = {San Diego, California},
 pages = {12--19},
 numpages = {8},
 acmid = {882087},
 publisher = {ACM},
 address = {New York, NY, USA},
} 


@inproceedings{RefWorks:82.48,
 author = {Papadimitriou, Spiros and Brockwell, Anthony and Faloutsos, Christos},
 title = {Adaptive, hands-off stream mining},
 booktitle = {Proceedings of the 29th international conference on Very large data bases - Volume 29},
 series = {VLDB '2003},
 year = {2003},
 isbn = {0-12-722442-4},
 location = {Berlin, Germany},
 pages = {560--571},
 numpages = {12},
 acmid = {1315500},
 publisher = {VLDB Endowment},
} 


@techreport{RefWorks:82.49,
      author       = "Perlman, E and Java, A",
      title        = "Predictive Mining of Time Series Data in Astronomy",
      number       = "astro-ph/0212413",
      month        = "Dec",
      year         = "2002",
}


@inproceedings{RefWorks:82.53,
 author = {Wang, Haixun and Fan, Wei and Yu, Philip S. and Han, Jiawei},
 title = {Mining concept-drifting data streams using ensemble classifiers},
 booktitle = {Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining},
 series = {KDD '03},
 year = {2003},
 isbn = {1-58113-737-0},
 location = {Washington, D.C.},
 pages = {226--235},
 numpages = {10},
 acmid = {956778},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {classifier, classifier ensemble, concept drift, data streams},
} 


@inproceedings{RefWorks:82.54,
 author = {Zhu, Yunyue and Shasha, Dennis},
 title = {StatStream: statistical monitoring of thousands of data streams in real time},
 booktitle = {Proceedings of the 28th international conference on Very Large Data Bases},
 series = {VLDB '02},
 year = {2002},
 location = {Hong Kong, China},
 pages = {358--369},
 numpages = {12},
 acmid = {1287401},
 publisher = {VLDB Endowment},
} 


@misc{RefWorks:86,
  author =       {John Lei and Ali Ghorbani},
  title =        {Improved Competitive Learning Neural Networks for Network Intrusion and Fraud Detection},
  abstract =     {In this research, we develop two new clustering algorithms, the Improved Competitive Learning Network (ICLN) and the Supervised Improved Competitive Learning Network (SICLN), for the applications in the area of fraud detection and network intrusion detection. The ICLN is an unsupervised clustering algorithm applying new rules to the the Standard Competitive Learning Neural Network(SCLN). In the ICLN, network neurons are trained to represent the center of the data by a new reward-punishment update rule. The new update rule overcomes the instability of the SCLN. The SICLN is a supervised clustering algorithm further developed from the ICLN by introducing supervised mechanism. In the SICLN, the new supervised update rule utilizes the data labels to guide the training process to achieve a better clustering result. The SICLN can be applied to both labelled and unlabelled data and is highly tolerant to missing or delay labels. Furthermore, the SICLN is completely independent from the initial number of clusters because it is able to reconstruct itself according to the labels of the cluster members. Experimental comparisons on both academic research data and practical real-world data for fraud detection and network intrusion detection demonstrated that the SICLN achieved high performance and outperformed traditional unsupervised clustering algorithms.},
  note = {{The authors have developed two new clustering algorithms,
                  the \textit{Improved Competitive Learning Network}
                  (ICLN) and the \textit{Supervised Improved
                  Competitive Learning Network} (SICLN), specifically
                  for use in the intrusion and fraud detection
                  domains. Data mining-based intrusion and fraud
                  detection is categorized into \textit{misuse
                  detection} and \textit{anomaly detection}. Misuse
                  detection is a classification exercise based on the
                  supervised learning from labelled data. Anomaly
                  detection establishes patterns of normal behaviour
                  and thereby identifies deviations. Both algorithms
                  are derived from the \textit{Standard Competitive
                  Learning Network} (SCLN).\\\\

\textbf{Standard Competitive Learning Network}\\
                  SCLN is a two layer neural network: distance measure
                  layer and competitive layer. During training, the
                  distance measure layer calculates the distance
                  between the weight vectors and the training
                  example. Of these distances, the competitive layer
                  finds the shortest and the winning weight vector is
                  adjusted (rewarded) towards the training
                  example. Eventually each of the weight vectors
                  converges towards the centroid of one
                  cluster. Clustering centers are the output of the
                  network. SCLN performance depends heavily on the
                  number of initial neurons and the initialization of
                  their weight vectors.\\\\

\textbf{Improved Competitive Learning Network}\\
                  ICLN changes the SCLN's reward only rule to include
                  a punishment for losing weight vectors. These
                  vectors are moved away from the training example
                  based on a kernel function and learning rate. This
                  change accelerates the learning process without
                  additional iterations.\\\\

                  ICLN initializes weight vectors to random training
                  examples or all to the mean of the training
                  data. ICLN can exclude redundant neurons via the
                  punishment rule, but not add to, the number of
                  clusters so the number of initial clusters is
                  usually set higher than expected.\\\\

                  During training, the ICLN iterates until the maximum
                  update to a weight vector falls below a minimum
                  update threshold or until a preset number of
                  iterations.\\\\

\textbf{Supervised Improved Competitive Learning Network}\\
                  SICLN modifies the ICLN learning rule to train on
                  both labelled and unlabelled data. It uses an
                  objective function to measure the quality of the
                  clustering result w.r.t the produced cluster centers
                  and the data set. The purpose of the objective
                  function is to optimize the purity and number of the
                  clusters.\\\\

                  SICLN is initialized in the same way as ICLN but
                  before learning begins, the initial weight vectors
                  of the network neurons are labelled with their
                  member data points. A weight vector is labelled to a
                  class if this class is the biggest population of the
                  members of this neuron of this weight vector. In the
                  case where only a portion of the data are labelled,
                  neurons may be labelled as "unknown" if no other
                  members of the same neuron are labelled.\\\\

                  During learning, SICLN uses labelled data, if
                  available, to update cluster centers. For labelled
                  training examples, only output
                  neurons that are the same class as the training
                  example or "unknown" class can compete. For
                  unlabelled training examples, SICLN functions
                  updates as per ICLN.\\\\

                  After the learning step, SICLN constructs a new
                  network. A neuron will be split in two if it
                  contains many members of other classes. Neurons are
                  merged if they belong to the same class. The
                  training step is repeated in this new
                  network. Training stops when then the objective
                  function or the number of iterations reaches
                  satisfies a certain threshold.\\\\

\textbf{Results}\\
                 ICLN and SICLN were compared with k-means and SOM
                  over three different data sets. ICLN exhibited
                  similar accuracy to the traditional
                  algorithms. SICLN outperformed all other algorithms
                  over all three data sets (\textbf{DOES SICLN QUALIFY
                  AS SEMI-SUPERVISED ?}\\\\
             }}
}


@misc{RefWorks:87,
  author =       {Majid Makki and Ali Ghorbani},
  year =         {2009},
  title =        {Ensemble of Word Clusters as the Feature Space for Document Clustering},
  abstract =     {This paper addresses the problem of dimensionality reduction in document clustering. A framework is proposed based on the mutual reinforcement of word clustering and document clustering. The idea is to initially cluster documents based on a subset of the original feature set and then expand the feature set using supervised distributional word clustering. Expectation-Maximization (EM) is employed to adopt the latter in an unsupervised realm. To overcome the high time complexity imposed by EM, parallelization is applied by means of sampling methods and unsupervised ensemble solving techniques. Four concrete versions of the framework are implemented and the behaviour of them are studied along with two rivals on three data sets. The upshot of the experiments is that the versions of the framework are comparable to their rivals on the two smaller data sets and better on the largest data set in terms of the trade-o they can make between processing time and accuracy. Moreover, it is shown that the results obtained by all the methods are roughly the same according to an internal evaluation measure.}
}


@misc{RefWorks:88,
  author =       {Zafarani, Reza and Ghorbani, Ali},
  title =        {Dynamic Clustering of Large Scale Data Using Random Sampling},
  abstract =     {In this paper, a new dynamic clustering algorithm based on random sampling is proposed. The algorithm addresses well known challenges in clustering such as Dynamism, Stability, and Scaling. The core of the proposed method is based on the definition of a function, named the Oracle, which can predict whether two random data points belong to the same cluster or not. Furthermore, this algorithm is also equipped with a novel technique for determination of the optimal number of clusters in datasets. These properties add the capabilities of high performance and reducing the effect of scale in datasets to this algorithm. Finally, the algorithm is tuned and evaluated by means of various experiments and in-depth analysis. High accuracy and performance results obtained, demonstrate the competitiveness of our algorithm.}
}

@misc{RefWorks:89,
  author =       {Reza Zafarani and Ali Ghorbani},
  title =        {Oracle Clustering: Dynamic Partitioning Based on Random Observations}
}


@inproceedings{RefWorks:90,
	author={Christos H. Papadimitriou and Hisao Tamaki and Prabhakar Raghavan and Santosh Vempala},
	year={1998},
	title={Latent semantic indexing: a probabilistic analysis},
	booktitle={Proceedings of the seventeenth ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems},
	series={PODS '98},
	publisher={ACM},
	address={New York, NY, USA},
	location={Seattle, Washington, United States},
	pages={159-168},
	isbn={0-89791-996-3},
        note={{The paper makes a formal analysis of
                  \textit{latent-semantic indexing} (LSI) in an
                  attempt to understand the apparent strengths of this
                  information retrieval technique. It has been
                  previously held that LSI captures the underlying
                  semantics of a corpus and is thereby able to
                  outperform more conventional vector-based methods
                  with reduced storage requirements and reduced query
                  times. These claims have thus far only been supported
                  by empirical evidence are proved formally, under
                  certain conditions, in the paper. Also, a
                  \textit{random projection} technique is proposed to
                  speed up LSI, which involves considerable
                  pre-processing. \\\\

                  Capturing the underlying (latent) semantics of
                  documents and queries during information retrieval
                  addresses the problems of \textit{synonymy} and
                  \textit{polysemy}. LSI attempts to capture this
                  hidden structure using linear algebra
                  techniques. The \textit{term-document} matrix is
                  reduced by \textit{single value decomposition}
                  (SVD). A portion of the largest singular values
                  are retained, forming a reduced matrix which allows
                  faster retrieval while still adequately capturing the
                  structure of the corpus. Therefore, LSI preserves
                  (to the extent possible) the relative distances (and
                  hence the retrieval capabilities) of the
                  term-document matrix while projecting it onto a lower
                  dimensional space.\\\\

                  During its formal analysis, the paper concludes that
                  the LSI transformation of the term-document matrix
                  aligns the vectors of documents on the same topic
                  and orthogonalizes vectors of documents on different
                  topics. This notion is confirmed during empirical
                  evaluations which measure the angle between intra-topic
                  and inter-topic documents. NOTE: from a clustering
                  perspective, would this not create tighter, better
                  separated structures ? Also, the paper describes how
                  synonymous terms would have very similar
                  representations in the term-document matrix and
                  would be "projected out" by the LSI transform, as
                  would be expected from a method that claims to
                  capture the semantics of the corpus.

                  The paper proposes a method for combining \textit{random
                  projection} with LSI. LSI is computationally
                  demanding and, in that respect, would benefit from
                  the reduction in dimension of the document space
                  provided by random projection.\\\\
        }}
}


@inproceedings{RefWorks:91,
	author={Margareta Ackerman and Shai Ben-David},
	year={2008},
	title={Measures of Clustering Quality: A Working Set of Axioms for Clustering},
	booktitle={Proceedings of the 22nd Annual Conference on Neural Information Systems Processing},
	abstract={{Aiming towards the development of a general clustering theory, we discuss abstract axiomatization for clustering. In this respect, we follow up on the work of Kleinberg, \cite{RefWorks:93} that showed an impossibility result for such axiomatization. We argue that an impossibility result is not an inherent feature of clustering, but rather, to a large extent, it is an artifact of the speciﬁc formalism used in \cite{RefWorks:93}. As opposed to previous work focusing on clustering functions, we propose to address clustering quality measures as the object to be axiomatized. We show that principles like those formulated in Kleinberg's axioms can be readily expressed in the latter framework without leading to inconsistency. A clustering-quality measure (CQM) is a function that, given a data set and its partition into clusters, returns a non-negative real number representing how strong or conclusive the clustering is. We analyze what clustering-quality measures should look like and introduce a set of requirements (axioms) for such measures. Our axioms capture the principles expressed by Kleinberg's axioms while retaining consistency. We propose several natural clustering quality measures, all satisfying the proposed axioms. In addition, we analyze the computational complexity of evaluating the quality of a given clustering and show that, for the proposed CQMs, it can be computed in polynomial time.}},
	keywords={clustering},
        note={{The paper analyses cluster quality measures in general
                  and proposes a set of requirements that well behaved
                  quality measures should exhibit. The paper is an
                  extension of \cite{RefWorks:93} in which Kleinberg
                  advocates the development of a cluster theory
                  independent of algorithm, objective function, or
                  generative model. The paper shows how Kleinberg's
                  \textit{cluster function} axioms, which he found to
                  be mutually inconsistent, can be transformed into
                  axioms about a \textit{clustering-quality
                  measure}. These transformed axioms are relaxed to
                  such a point that they are no longer
                  inconsistent.\\\\ 

                  The axioms proposed are as follows:\\
\begin{itemize}
\item \textbf{Scale Invariance}: the quality measure is not effected
                  by uniform scaling of the distance function output.

\item \textbf{Isomorphism Invariance}: the quality measure is not
                  effected by the by the individual identity of the
                  clustered elements i.e. a permutation on point
                  labels should not effect the output of a clustering
                  function. 

\item \textbf{Weak Local Consistency}: the quality measure does not
                  change value if the distances between pairs of
                  points within each cluster shrink , and distances
                  between pairs of points in different clusters
                  expands. This shrinkage and expansion need not occur
                  uniformly. 

\item \textbf{Co-Final Richness}: requires that the quality of any
                  clustering can be improved arbitrarily via
                  consistent changes of the distance function.
\end{itemize}

                  \bigskip
                  In order for any clustering-quality measure to be
                  considered good, it must satisfy all proposed
                  axioms. Clustering-quality measures for various
                  common cluster paradigms are proposed and analyzed in
                  the context of these axioms: \textit{loss-based},
                  \textit{center-based}, and
                  \textit{linkage-based}.\\\\ 
        }}
}


@inproceedings{RefWorks:92,
	author={Ellen M. Voorhees},
	year={1985},
	title={The cluster hypothesis revisited},
	booktitle={Proceedings of the 8th annual international ACM SIGIR conference on Research and development in information retrieval},
	series={SIGIR '85},
	publisher={ACM},
	address={New York, NY, USA},
	location={Montreal, Quebec, Canada},
	pages={188-196},
	isbn={0-89791-159-8},
        note={{The paper revisits the notion
                  proposed by \cite{RefWorks:94} that not only can
                  clustering be used to reduce the number of documents
                  which have to be compared to the query, but that the
                  clusters themselves embody useful information which
                  can be exploited to improve the effectiveness as
                  well as the efficiency of a retrieval
                  search. Specifically, \cite{RefWorks:94} stated the
                  \textit{cluster hypothesis}: documents that are
                  similar to one another (by some clustering distance
                  function) are relevant to the same queries, and
                  proposed \textit{cluster-based retrieval} to exploit
                  this relationship. Cluster-based retrieval retrieves
                  one or more clusters in their entirety in response
                  to a query, as opposed to most other cluster methods
                  which identify clusters which are are likely to
                  contain good documents and then compute the
                  similarity between the query and each of the
                  documents in the identified clusters.\\\\

                  Research involving relevance feedback by
                  \cite{RefWorks:95}, brings the cluster hypothesis
                  into question and thereby the effectiveness of
                  cluster-based retrieval. This paper proposes a new test as to
                  whether the cluster hypothesis holds for a given
                  document collection and conducts cluster-based and
                  non-cluster-based searches on these document
                  collections to empirically examine the validity of
                  the hypothesis.\\\\

                  The original cluster hypothesis test plots and
                  compares frequency distributions of pairwise
                  similarity between documents relevant to a
                  particular query vs between documents which are not
                  relevant to the same query. The paper regards this
                  test as not sufficiently granular and proposes a new
                  test based on relevance of a fixed number of nearest
                  neighbours.\\\\

                  By comparing the assessment of the new cluster
                  hypothesis test to the relative performance of
                  hierarchical cluster-based searches with sequential
                  searches, the paper concludes the the relative
                  performance seems to be
                  independent of how well the cluster hypothesis
                  characterizes the collection. Also, the paper
                  concludes that cluster searches that retrieve
                  individual documents almost always performed better
                  than a cluster-based search (returns entire
                  cluster).\\\\

                  I think, given the conclusions above, the paper's claims that its
                  cluster-hypothesis test is an improvement over
                  \cite{RefWorks:93} are weak. The results of the
                  research brings into question whether the cluster
                  hypothesis holds universally at all or whether it is
                  largely dependent on the choice of our notion of
                  similarity (document compared to document) vs
                  relevance (document compared to query).\\\\
        }}
}


@misc{RefWorks:93,
  author = 	 {J. Kleinberg},
  year = 	 {2002},
  title = 	 {An impossibility theorem for clustering},
  abstract = 	 {Although the study of clustering is centered around an intuitively compelling goal, it has been very di\#cult to develop a unified framework for reasoning about it at a technical level, and profoundly diverse approaches to clustering abound in the research community. Here we suggest a formal perspective on the di\#culty in finding such a unification, in the form of an impossibility theorem: for a set of three simple properties, we show that there is no clustering function satisfying all...}},
  keywords = 	 {clustering; impossibility},
  url = 	 {http://citeseer.ist.psu.edu/kleinberg02impossibility.html}
}


@article{RefWorks:94,
	author={N. Jardine and C. J. van Rijnsbergen},
	year={1971},
	title={The Use of Hierarchical Clustering in Information Retrieval},
	journal={Inform. Stor. \& Retr},
	number={7},
	pages={217-240}
}


@misc{RefWorks:95,
	author={Eleanor Rose Cook Ide},
	year={1969},
	title={Relevance feedback in an automatic document retrieval system},
	journal={Report ISR-I5 to the National Science Foundation}
}


@article{RefWorks:96,
	author={Todd A. Letsche and Michael W. Berry},
	year={1997},
	month={8},
	title={Large-scale information retrieval with latent semantic indexing},
	journal={Information Sciences},
	volume={100},
	number={1-4},
	pages={105-137},
	isbn={0020-0255},
        note={{The paper describes the implementation of a \textit{latent
                  semantic indexing} (LSI) search function library
                  called LSI++. The library is implemented in C++ and
                  designed with a modular API to facilitate easy
                  integration into other applications. A test
                  application incorporating LSI++ and exposing a WWW
                  interface was built by the authors and is used to
                  benchmark the implementation against a previous
                  implementation at Bellcore \cite{RefWorks:102}. In serial mode, the
                  new system was found to be six times faster than the
                  Bellcore system. These improvements can be accounted
                  for by more efficient programming. The authors also
                  implemented a parallel version of the LSI search
                  function and managed to achieve nearly 180 times
                  improvement over the previous implementation over
                  some document collections.\\\\

                  During the search phase of LSI, each vector of the
                  term-document space must be loaded into memory and
                  compared to the query \textit{pseudo-document}
                  vector. Because of finite memory capacity, it is not
                  always possible to load the entire collection of
                  document vectors into primary storage and vectors
                  must be loaded and unloaded to and from main memory
                  as the search progresses. It is this transfer to and
                  from primary storage that the authors direct their
                  optimizations. The collection of document vectors
                  are partitioned between a number of workstations,
                  each loading the entire partition into main
                  memory. A root node receives the search request,
                  creates the query pseudo document and broadcasts it
                  to the other members. Each workstation then
                  computes, in parallel, the similarity measure
                  between the pseudo document and each local vector
                  and returns the results to the root node which
                  performs a global sort and returns a ranked list to
                  the user. An interesting result of benchmark testing
                  is that adding more processors to the smaller
                  document collections generally did not have a great
                  effect but larger collections benefited
                  significantly from increased parallelism. This is
                  probably because smaller collections are able to fit
                  a larger portion of the document representation
                  collection in primary storage and do not suffer the
                  paging effect to the same extent as the larger
                  collections. Also, the overhead imposed by
                  distributing and reconstituting the search across
                  the workstations is relatively insignificant for
                  large collections but not so for small collections.\\\\

                  Although LSI is capable of achieving significant
                  retrieval performance gains over standard lexical
                  techniques, it's execution efficiency lags behind
                  these simpler methods and can become prohibitively
                  slow on very large data sets. However, most of the
                  processing effort can be attributed to the
                  pre-processing phase which the the paper does not
                  address. The system described in the paper assumes
                  that this pre-processing is a one-time cost and that
                  software to perform this step already exists. This
                  point of view holds for static evaluation corpora,
                  however, by the authors own admission, a functional
                  real-world system would require some method of
                  updating and downdating the document set without
                  having to pre-process the entire document collection
                  from scratch. Also, speeding up the search process
                  through concurrent execution obviously yields
                  improvements however, this method can be applied to
                  any retrieval algorithm which performs an exhaustive
                  search on a document collection, decomposed or
                  not.\\\\}}
}


@misc{RefWorks:97,
  author = 	 {Kirk Baker},
  year = 	 {2008},
  title = 	 {Singular Value Decomposition Tutorial},
  note={{This is tutorial covering vector and matrix methods in
                  support of explaining the mechanics
                  of \textit{singular value decomposition} and its use
                  in \textit{information retrieval}. It describes
                  vectors in terms of points in a three dimensional
                  space and then extends the definition to an
                  arbitrary number of dimensions. This n dimensional
                  space is called textit{hyperspace}
                  or \textit{n-space}. The hyperspace document
                  representation is described as a method of
                  representing a document as a vector whose components
                  correspond in some way to the words occurring in the
                  document. The tutorial details the primary vector
                  operations: \textit{addition}, \textit{scalar
                  multiplication}, and \textit{inner product} (also
                  called the \textit{dot product} or \textit{scalar
                  product}). Vectors are \textit{orthogonal} to one
                  another if their inner product is zero. In
                  two-dimensional space, this is equivalent to saying
                  that the angle between them is 90 degrees. The
                  implications of this to document retrieval is that
                  the \textit{cosine distance} between two such
                  vectors is zero. The \textit{Gram-Schmidt} process
                  for converting a set of vectors
                  into \textit{orthonormal vectors} (orthogonal unit
                  vectors) is described.\\\\

                  Matrices are introduced as collections of vectors
                  and the general notation thereof is covered in
                  detail. The basic operations of multiplication and
                  transposing matrices are described. A matrix is
                  orthogonal if multiplying it by its transpose
                  produces the identity matrix. Intuitively, if the
                  vectors are all orthogonal unit vectors, the
                  diagonal elements, produced by the inner product of
                  each vector with itself, will be one, and all other
                  entries will be zero. Calculation of the
                  matrix \textit{determinant} is described but its
                  significance is not
                  explained. Also, \textit{eigenvalues}
                  and \textit{eigenvectors} are covered briefly and a
                  sample calculation thereof shown. Intuitively,
                  eigenvectors of a rotational matrix are those
                  vectors which will be scaled but not rotated by the
                  transform. The amount of scaling that occurs is the
                  eigenvalue.\\\\

                  The paper concludes with a description of singular
                  value decomposition (SVD) and gives some useful
                  illustrations to convey how the process may
                  interpreted. In the context of document retrieval,
                  SVD breaks down the term-document matrix into
                  linearly independent (orthogonal) components along
                  which most variation occurs and reduces noisy
                  correlations between documents
                  (In \cite{RefWorks:101}, this process is interpreted
                  as introducing noise, not removing it. I am inclined
                  to go with the published
                  paper \cite{RefWorks:101}). If the components
                  exhibiting the smallest variation in this new
                  representation are eliminated, we end up with a
                  reduced dimension approximation which best captures
                  the original data. This feature reduction makes
                  documents which share common substructure (topic)
                  more similar and those which do not, more
                  dissimilar. In practical terms, this means that
                  documents about a particular topic become more
                  similar in the new representation, even if the exact
                  same words don't appear in all of them.\\\\
       }}
}


@article{RefWorks:99,
	author={Tamara G. Kolda and Dianne P. O'Leary},
	year={1998},
	month={October},
	title={A semidiscrete matrix decomposition for latent semantic indexing information retrieval},
	journal={ACM Trans.Inf.Syst.},
	volume={16},
	number={4},
	pages={322-346},
	keywords={data mining; latent semantic indexing; semidiscrete decomposition; singular-value decomposition; text retrieval},
	isbn={1046-8188},
        note={{The paper proposes a \textit{semi-discrete
                  decomposition} (SDD) to replace \textit{singular
                  value decomposition} (SVD) as used in \textit{latent
                  semantic indexing} (LSI) for approximating a
                  document matrix. LSI has been shown to overcome many
                  difficulties associated with literal term matching
                  and the method proposed in the paper claims to
                  exceed the performance thereof in certain aspects.\\\\

                  As background, the paper describes the
                  \textit{vector space method} for document retrieval,
                  of which LSI (both SVD- and SDD-based methods) is an
                  extension. The creation of the \textit{term-document
                  matrix} is described and extensive coverage is given
                  to the different methods of weighting terms in the
                  document matrix. A term weight has three components:\\

\begin{itemize}
\item \textbf{local weight}: based on the occurrence of the term
                  within a specific document.
\item \textbf{global weight}: based on the the occurrence of the term
                  within the document collection as a whole.
\item \textbf{normalization}: specifies whether or not the columns
                  (documents) are normalized.
\end{itemize}
\bigskip

                  When executing queries, weightings are also applied
                  to the \textit{pseudo document} vector, though these
                  need not be the same as the term
                  weightings. Normalizing the query vector has no
                  effect on document ranking so it is never done.\\\\

                  The paper gives a good illustration as to how
                  queries based on both SVD and SDD approximations of
                  the term-document matrix can actually improve
                  document retrieval. The decomposition introduces
                  'noise' as the rank of the original matrix is
                  reduced. This noise makes similar documents appear
                  more similar while remaining close to the original
                  data. The amount of noise introduced depends on the
                  extent to which the rank of the original matrix is
                  compressed.\\\\

                  The SDD method proposed was originally introduced by
                  \cite{RefWorks:99} and is used here in a bid to
                  save storage space and query time. The method does
                  not reproduce the original matrix exactly, even with
                  no rank compression, but for equal reduction of
                  rank, the SVD method requires nearly 32 times more
                  storage than SDD. SDD is calculated iteratively,
                  terminating when the improvement in a residual
                  stagnates. Queries using the SDD decomposition use
                  fewer floating point operations than queries on an
                  SVD decomposition.\\\\
                  
                  An empirical comparison of the vector space method,
                  SVD-based LSI, and SDD-based LSI, was conducted over
                  three different corpora. The results indicate that
                  SDD-based LSI retrieves documents as well as
                  SVD-based LSI but requires only about half the query
                  time. The disadvantage of the SDD method is that it
                  takes five times as long as computing the SVD
                  approximation.\\\\
                  
                  The paper proposes some techniques for updating the
                  SDD approximation when the document collection
                  changes as well as methods to improve the
                  decomposition if it is found to be inadequate (in
                  terms of retrieval accuracy). The SDD update methods
                  are easier than updating SVD. SVD update methods can
                  take as long as the original decomposition, but
                  require less memory.\\\\
                  
                  The corpora used in the evaluation are relatively
                  small (<2000 documents) and it remains to be seen
                  how SDD-based LSI performs on large
                  collections. Given the large initial approximation
                  computation time, SDD may not scale adequately to be
                  of practical use in such situations.\\\\
        }}
}

@article{RefWorks:100,
	author={Tao Li and Qi Li and Shenghuo Zhu and Mitsunori Ogihara},
	year={2002},
	month={December},
	title={A survey on wavelet applications in data mining},
	journal={SIGKDD Explor.Newsl.},
	volume={4},
	number={2},
	pages={49-68},
	isbn={1931-0145}
}


@article{RefWorks:101,
	author={Hongyuan Zha and Horst D. Simon},
	year={1999},
	title={On Updating Problems in Latent Semantic Indexing},
	journal={SIAM Journal on Scientific Computing},
	volume={21},
	number={2},
	pages={782-791},
	keywords={singular value decomposition; updating problems; latent semantic indexing; information retrieval},
        note={{The paper describes a new \textit{singular value
                 decomposition} (SVD) updating algorithm to support
                 the use of \textit{latent semantic indexing} (LSI)
                 on evolving document collections. LSI is a type of
                 vector space information retrieval method that uses
                 a \textit{reduced dimension representation} (RDR)
                 computed by the SVD of the \textit{term-document}
                 matrix. For each of the dimensions reduced,
                 a \textit{pseudoconcept} is introduced which may not
                 have any explicit semantic content but which helps to
                 discriminate documents.\\\\

                  Three types of updating problems exist in LSI:\\
\begin{itemize}
\item \textbf{Updating documents}: documents are added to the collection.
\item \textbf{Updating terms}: new terms are added to the model.
\item \textbf{Term weight corrections}: term weights in the original term-document matrix need to be adjusted.
\end{itemize}
                
                \bigskip
                A method for updating the LSI-generated RDR,
                called \textit{folding}, is described
                in \cite{RefWorks:102}. However, this method is based
                on the original RDR and does not adjust the
                representation of existing terms and documents,
                causing retrieval accuracy to suffer. This paper
                focuses on the three SVD updating methods derived
                in \cite{RefWorks:105,RefWorks:107} and proposes
                enhancements thereto. The enhanced methods are proven
                formally to be more accuracy approximations of the
                original term-document matrix, albeit at additional
                computation cost. The paper also makes a formal
                justification for using the RDR as the basis for
                updating, as opposed to the original term-document
                matrix. The proof shows that the information lost in
                the RDR would also be lost in decomposing an updated
                term-document matrix. This result has important
                practical consequences as, in many cases, the original
                term-document matrix is not maintained on disk or in
                memory.\\\\

                The new update methods are tested on three document
                collections and compared against the methods
                from \cite{RefWorks:105,RefWorks:107} as well as an
                update method based on the entire term-document
                matrix. In the tests, the document collection is
                broken into partitions and the collection updated
                incrementally and the accuracy of queries against the
                updated RDRs are compared. For one document
                collection, the new method shows significant
                improvement over \cite{RefWorks:105,RefWorks:107} and
                marginally exceeds the performance of the update
                method based on the entire term-document matrix. For
                the other document collections, the new method shows
                marginal improvement
                over \cite{RefWorks:105,RefWorks:107} and is
                comparable to the update method based on the entire
                term-document matrix. In all cases, the computation
                cost for the new method is higher than that
                of \cite{RefWorks:105,RefWorks:107}.\\\\

                As is the case with many LSI-based studies, the test
                collections are small and do not qualify the results
                for large-scale practical use. However the formal
                proofs in this paper provide some confidence in the
                basis of the decompositions used and how accurately we
                can expect them to approximate the original
                corpus.\\\\
        }}
}


@article{RefWorks:102,
	author={Scott Deerwester and Susan T. Dumais and George W. Furnas and Thomas K. Landauer and Richard Harshman},
	year={1990},
	title={Indexing by latent semantic analysis},
	journal={Journal of the American Society for Information Science},
	volume={41},
	number={6},
	pages={391-407},
	abstract={Abstract A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents (“semantic structure”) in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca. 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca. 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are returned. Initial tests find this completely automatic method for retrieval to be promising. © 1990 John Wiley & Sons, Inc.},
	isbn={1097-4571}
}


@article{RefWorks:103,
	author={Alfred V. Aho and Margaret J. Corasick},
	year={1975},
	month={June},
	title={Efficient string matching: an aid to bibliographic search},
	journal={Commun.ACM},
	volume={18},
	number={6},
	pages={333-340},
	keywords={bibliographic search; computational complexity; finite state machines; information retrieval; keywords and phrases; string pattern matching; text-editing},
	isbn={0001-0782}
}


@article{RefWorks:105,
	author={Michael W. Berry and Susan T. Dumais and Gavin W. O'Brien},
	year={1995},
	title={Using Linear Algebra for Intelligent Information Retrieval},
	journal={SIAM Review},
	volume={37},
	number={4},
	pages={573-595},
	keywords={indexing; information; latent; matrices; retrieval; semantic; singular value decomposition; sparse; updating},
}


@mastersthesis{RefWorks:107,
	author={G. W. O'Brien},
	year={1994},
        school={Department of Computer Science, University of Tennessee},
        address={Knoxville, TN}
	title={{Information Management Tools for Updating an SVD-Encoded Indexing Scheme}}
}


@book{RefWorks:109,
	author={Christopher D. Manning and Prabhakar Raghavan and Hinrich Schtze},
	year={2008},
	title={Introduction to Information Retrieval},
	publisher={Cambridge University Press},
	address={New York, NY, USA},
	isbn={0521865719, 9780521865715}
}


@inproceedings{RefWorks:126,
	author={Xiaojun Wan and Jianwu Yang},
	year={2007},
	title={Single document summarization with document expansion},
	booktitle={Proceedings of the 22nd national conference on Artificial intelligence - Volume 1},
	publisher={AAAI Press},
	location={Vancouver, British Columbia, Canada},
	pages={931-936},
	isbn={978-1-57735-323-2},
        note={{The paper proposes a document summarization technique
                  that makes use of document expansion to improve
                  single document summarization. The technique focuses
                  on generic, as opposed to query-specific,
                  summarization and is based on the assumption that
                  topic-related documents can provided an enlarged
                  context within which a better summary may be
                  extracted from a specific document.\\\\

                  The proposed approach begins by expanding the target
                  document into a small document set of the \textit{k}
                  nearest neighbors of the target. In this case, the
                  similarity measure used in the the standard
                  \textit{cosine Measure}.\\\\

                  Given the sentence collection of the document set,
                  the \textit{affinity} weight between pairs of
                  sentences is calculated using the cosine measure. In
                  this calculation, sentences are represented as
                  weighted vectors where the weights are the product
                  of term frequency within the sentence and inverse
                  sentence frequency. Then an \textit{affinity graph}
                  is constructed in which sentences are represented as
                  nodes, and edges reflect a non-zero affinity weight
                  between sentences. Edges are weighted based on
                  affinity weight and the \textit{cosine distance}
                  between the sentences. For comparison sake, two
                  additional graphs were constructed at this point,
                  containing only inter-document links and
                  intra-document links respectively.\\\\

                  Based on the \textit{affinity graphs}, the
                  \textit{informativeness} score of each sentence is
                  calculated iteratively, based on the scores of those
                  sentences linked to it. A greedy ranking algorithm
                  is then applied which penalizes sentences based on
                  redundancy. The highest ranked sentences are
                  supposedly highly informative and novel and are
                  included in the final document summary.\\\\

                  Empirical evaluation of this method on the DUC2002
                  corpus, indicates that document expansion does
                  benefit single document summarization. Also, using
                  inter-document links alone produces better results
                  than using inter-link documents to construct
                  summaries. The size of the expanded document set was
                  found to be optimal at less than ten documents, an
                  encouraging result given the added computational
                  complexity over the single document method. The
                  paper suggests using clustering to derive document
                  sets, as a possible optimization.\\\\

                  These results lend themselves to the notion that
                  inter-document relations reflect latent document
                  semantics.\\\\ }}
}

@inproceedings{RefWorks:127,
	author={Wen-tau Yih and Joshua Goodman and Lucy Vanderwende and Hisami Suzuki},
	year={2007},
	title={Multi-document summarization by maximizing informative content-words},
	booktitle={Proceedings of the 20th international joint conference on Artifical intelligence},
	publisher={Morgan Kaufmann Publishers Inc},
	address={San Francisco, CA, USA},
	location={Hyderabad, India},
	pages={1776-1782},
        note={{The paper describes a new multi-document summarization
                  technique which enhances the simple yet very
                  effective \textit{SumBasic}
                  system. \textit{SumBasic} computes the probability
                  of content words within the document set and scores
                  individual sentences based on the average
                  probabilities of the constituent words. The summary
                  is generated by a greedy algorithm that iteratively
                  selects the sentences with the highest scoring
                  content word, breaking ties using the average
                  sentence scores. This continues until the maximum
                  summary length is reached.\\\\

                  The proposed system differs from the \textit{SumBasic}
                  implementation in the following ways:\\
\begin{itemize}
\item \textbf{Scoring:} enhanced sentence scoring method using a
                  discriminative machine learning algorithm to combine
                  term position information with term frequency. Seven
                  features are defined using frequency and position
                  information and the model is trained using human
                  generated summaries to predict the probability of a
                  term appearing in a summary.
\item \textbf{Sentence Selection:} a more complex algorithm than that
                  of \textit{SumBasic} is used to select sentences fof the
                  summary. This algorithm, based on a stack decoder,
                  searches explicitly for the combination of sentences
                  which will maximize the sum of term scores for a
                  specific sentence.
\item \textbf{Sentence Simplification:} sentences are simplified to
                  remove phrases with little expected value. The
                  simplified sentences compete for selection by the
                  summarizer with the original sentences.
\end{itemize}

                  \bigskip The technique proposed in the paper gives
                  the best results reported on the DUC-2004 and MSE-05
                  summarization tasks for the ROUGE-1 score, although
                  only the for the DUC-2004 task is the difference
                  statistically significant. The algorithm is considered
                  significantly simpler than the previous best
                  system. Sentence simplification seems to have a
                  small positive effect for certain but not all
                  evaluation scores.\\\\ }}
}


@inproceedings{RefWorks:128,
	author={Matei Zaharia and Andy Konwinski and Anthony D. Joseph and Randy Katz and Ion Stoica},
	year={2008},
	title={Improving MapReduce performance in heterogeneous environments},
	booktitle={Proceedings of the 8th USENIX conference on Operating systems design and implementation},
	series={OSDI'08},
	publisher={USENIX Association},
	address={Berkeley, CA, USA},
	location={San Diego, California},
	pages={29-42},
        note={{The paper analyses the scheduling method used in
                  Hadoop, a distributed \textit{\textit{MapReduce}}
                  implementation and shows that, under certain
                  conditions, it can result in severe performance
                  degradation. The paper proposes an enhanced
                  scheduling algorithm, LATE, that addresses some of
                  these issues and details a series of experiments
                  conducted to compare the two algorithms.\\\\

                  \textit{\textit{MapReduce}} is a distributed
                  framework for parallelizing the execution of a task
                  across multiple nodes. Hadoop is a popular
                  implementation of \textit{MapReduce} that was
                  developed primarily by Yahoo. During execution of a
                  task, \textit{MapReduce} will attempt to identify
                  \textit{stragglers}, subtasks which have fallen
                  behind the pace or failed. \textit{MapReduce} will
                  run speculative copies of stragglers on other nodes
                  in order to complete the computation faster. This
                  paper contends that the Hadoop scheduler does not
                  take into account the fact that nodes may not
                  perform work at equal rates and that tasks may not
                  progress at a constant rate. Variance in these
                  measures can cause the Hadoop scheduler to launch
                  excessive speculation tasks. Since the scheduler
                  does not take into account the cost of speculation
                  and does not place an upper limit on their number,
                  speculative tasks can aggravate a struggling
                  system.\\\\

                  The LATE (Longest Approximate Time to End) scheduler
                  aims to improve upon the native Hadoop scheduler by
                  trying to speculatively execute the task expected to
                  finish farthest into the future. The impetus being
                  that this provides the greatest opportunity for a
                  speculative task to overtake the original and reduce
                  overall execution time. LATE uses simple heuristics
                  to estimate the time left before a task completes,
                  which do not seem immune to the shortcomings of the
                  native scheduler. By the author's own admission,
                  non-uniform task progress can cause incorrect
                  extrapolation of completion time with LATE and
                  result in a priority inversion. The value of LATE
                  seems to lie in the fact that it places a cap on the
                  number of speculation tasks and will only target
                  speculative tasks at nodes that are performing
                  adequately.\\\\

                  Experiments comparing the native Hadoop scheduler, a
                  non-speculative scheduling strategy, and LATE were
                  conducted using hosted virtual machines (VMs) in the
                  Amazon EC2 environment as well as a dedicated local
                  cluster. Variance in node performance was introduced
                  by varying the distribution of VMs across physical
                  machines and also introducing network and disk
                  transfer loads. Results show speculative execution
                  (native Hadoop and LATE) outperforming
                  non-speculative execution and LATE outperforming
                  native Hadoop in most cases. The tests were not
                  exhaustive and results exhibited high variance,
                  bringing into question their statistical
                  significance. The paper introduces some interesting
                  ideas but does not explore them fully, focusing
                  excessively on description of the test environment,
                  to the detriment of the analysis of the proposed
                  algorithm.\\\\
             }}
}
